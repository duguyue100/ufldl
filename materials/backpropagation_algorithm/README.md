Backpropagation Algorithm
=====

###Introduction###

This folder consists several visualisations and implementations of the materials.

The original tutorial is presented at:

http://deeplearning.stanford.edu/wiki/index.php/Backpropagation_Algorithm

###Updates###

1. Update BP algorithm [20140319]

###Notes###

1. Regularization term/weight decay term tends to decrease the magnitude of the weights, and helps prevent overfitting.
2. The wight decay parameter `lambda` controls the relative importance of two terms.
3. Each `W` and `b` are initialized to a small random value near zero (`Normal(0, epsilon^2)` distribution for some small `epsilon`, say 0.01).
4. The random initialization serves the purposes of _symmetry breaking_.
5. The result seems not good at all, need to examine the reason.

###Contacts###

Hu Yuhuang

_No. 42, North, Flatland_

Email: duguyue100@gmail.com
